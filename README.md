# Project Title: Implementing and Applying Stable Diffusion Models

## 1. Introduction
Briefly describe the project: its goal is to explore Stable Diffusion, both by implementing core components from scratch and by using pre-built Hugging Face pipelines for a practical application. Mention this was for the "Advanced Learning INE2-DATA 2025 - Deep Learning Lab" project.

## 2. Project Structure
Explain the two main parts of the project and their corresponding notebooks:
* **Part 1: Stable Diffusion from Scratch (`stable_diffusion_scratch (2).ipynb`)**
    * Objective: To understand the fundamental building blocks of Stable Diffusion (VAE, CLIP, UNet, Sampler).
    * Briefly mention key components implemented.
* **Part 2: Story-to-Storyboard with Hugging Face (`storyline (1).ipynb`)**
    * Objective: To use high-level APIs (`diffusers` library) to generate a sequence of images from a story.
    * Briefly describe the workflow (story segmentation, prompt generation, image generation).

## 3. Environment Setup
* Python version (e.g., Python 3.9+).
* **Dependencies**:
    * "All dependencies are listed in `requirements.txt`. Install them using:"
    * `pip install -r requirements.txt`
* **Model Checkpoints**:
    * For `stable_diffusion_scratch (2).ipynb`:
        * "Download the Stable Diffusion v1.5 checkpoint (`v1-5-pruned-emaonly.ckpt`) from [Link to Hugging Face model or source, e.g., https://huggingface.co/runwayml/stable-diffusion-v1-5/tree/main]."
        * "Place it in the `data/` directory (or specify your chosen path)."
        * "Ensure you have the CLIP tokenizer files (`vocab.json`, `merges.txt`) available and provide their path if necessary or use `CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')` as a fallback if local files aren't found."
    * For `storyline (1).ipynb`:
        * "The Hugging Face `diffusers` library will automatically download the required model (e.g., `runwayml/stable-diffusion-v1-5` or `stabilityai/stable-diffusion-2-1-base`) on first run. An internet connection is required."

## 4. How to Run the Notebooks
* Instructions for `stable_diffusion_scratch (2).ipynb`:
    * "Ensure the checkpoint and tokenizer are set up as described above."
    * "Open and run the cells sequentially."
* Instructions for `storyline (1).ipynb`:
    * "Open and run the cells sequentially. The first run might take time to download models."
    * "You can modify the `story_text` variable to generate storyboards for different narratives."

## 5. Source Code Documentation
* You can either:
    * Briefly describe the main classes and functions here, referring to the detailed comments and Markdown within the notebooks themselves.
    * Or, include more detailed explanations of key components, drawing from the Markdown documentation we've generated (e.g., a short paragraph on the UNet architecture, the DDPMSampler logic, the VAE, CLIP, and the main generation pipeline functions).

## 6. Example Outputs (Optional but Recommended)
* Include one or two example images generated by each notebook. You can embed images in Markdown.

## 7. Video Presentation Link
* Once your video is ready (e.g., uploaded to YouTube or another platform), include the link here.
